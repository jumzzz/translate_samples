Kapag masira mo o maling i-configure ang code ay madalas kang makakakuha ng ilang uri ng isang pagbubukod. Naka-plug ka sa isang integer kung saan inaasahan ng isang string. Inaasahan lamang ng pag-andar ang 3 argumento. Nabigo ang pag-import na ito. Ang susi na iyon ay hindi umiiral. Ang bilang ng mga elemento sa dalawang listahan ay hindi katumbas. Bilang karagdagan, madalas na posible na lumikha ng mga pagsusulit ng yunit para sa isang tiyak na pag-andar.

Ito ay isang panimula lamang pagdating sa pagsasanay neural nets. Ang lahat ay maaaring tama syntactically, ngunit ang buong bagay ay hindi maayos na nakaayos, at ito ay talagang mahirap sabihin. Ang “posibleng ibabaw ng error” ay malaki, lohikal (kumpara sa syntactic), at napaka nakakalito sa yunit ng pagsubok. Halimbawa, marahil nakalimutan mong i-flip ang iyong mga label kapag kaliwa-kanan mo binaligtad ang imahe sa panahon ng pagpapalaki ng data. Ang iyong net ay maaari pa ring (shockingly) gumana medyo maayos dahil ang iyong network ay maaaring panloob na matuto upang makita ang mga nabaligtad na imahe at pagkatapos ito kaliwa-kanan flips ang mga hula nito. O marahil ang iyong autoregressive modelo sinasadyang tumatagal ang bagay na sinusubukan upang mahulaan bilang isang input dahil sa isang off-by-one bug. O sinubukan mong i-clip ang iyong gradients ngunit sa halip pinutol ang pagkawala, nagiging sanhi ng outlier halimbawa na hindi papansinin sa panahon ng pagsasanay. O pinasimulan mo ang iyong mga timbang mula sa isang pretrained checkpoint ngunit hindi ginamit ang orihinal na ibig sabihin. O pinutol mo lang ang mga setting para sa mga lakas ng regularization, rate ng pag-aaral, rate ng pagkabulok nito, laki ng modelo, atbp Samakatuwid, ang iyong maling na-configure neural net ay magtatapon lamang ng mga pagbubukod kung ikaw ay masuwerteng; Karamihan sa mga oras na ito ay sanayin ngunit tahimik na gumana nang kaunti mas masahol pa.

Bilang isang resulta, (at ito ay talagang mahirap na labis na bigyang-diin) ang isang “mabilis at galit na galit” na diskarte sa pagsasanay ng mga neural network ay hindi gumagana at humahantong lamang sa paghihirap. Ngayon, ang paghihirap ay isang perpektong likas na bahagi ng pagkuha ng isang neural network upang gumana nang maayos, ngunit maaari itong mapawi sa pamamagitan ng pagiging masinsinang, nagtatanggol, paranoyd, at nahuhumaling sa mga visualization ng karaniwang bawat posibleng bagay. Ang mga katangian na sa aking karanasan ay higit na nauugnay sa tagumpay sa malalim na pag-aaral ay ang pasensya at pansin sa detalye.